{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0adbd93",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a823490",
   "metadata": {},
   "source": [
    "### 5 diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed10cc",
   "metadata": {},
   "source": [
    "- Abordagem de construção do modelo: O AdaBoost ajusta os pesos das instâncias dos dados a cada iteração para criar uma série de modelos fracos que são combinados para formar um modelo forte. Por outro lado, o GBM ajusta os resíduos dos modelos anteriores a cada iteração, criando uma série de modelos sucessivos que tentam corrigir os erros dos modelos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639becfb",
   "metadata": {},
   "source": [
    "- Função de perda: O AdaBoost utiliza uma função de perda exponencial que é sensível a outliers. Em contraste, o GBM pode usar várias funções de perda, como a função de perda de desvio absoluto (L1) ou a função de perda de erro quadrático médio (L2), que são menos sensíveis a outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bef64",
   "metadata": {},
   "source": [
    "-  Número de hiperparâmetros: O AdaBoost tem menos hiperparâmetros para ajustar, geralmente apenas um parâmetro de aprendizado que controla a taxa de aprendizado. O GBM tem mais hiperparâmetros para ajustar, como o número de árvores de decisão, a profundidade máxima da árvore, a taxa de aprendizado e a fração de amostras utilizadas para cada árvore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287a1de",
   "metadata": {},
   "source": [
    "- Sensibilidade ao ruído: O AdaBoost é mais sensível ao ruído nos dados de treinamento do que o GBM, pois os erros em instâncias individuais têm um grande efeito no ajuste de pesos. O GBM, por outro lado, pode lidar melhor com dados ruidosos, já que ele se concentra em ajustar os resíduos dos modelos anteriores, em vez de ajustar diretamente os dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12a7b8",
   "metadata": {},
   "source": [
    "- Velocidade de treinamento: O AdaBoost é mais rápido para treinar do que o GBM, pois ele constrói modelos fracos simples que exigem menos recursos computacionais. Em contraste, o GBM constrói modelos sucessivos mais complexos que exigem mais tempo de processamento para treinar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b0cef",
   "metadata": {},
   "source": [
    "## Exemplo GBM Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d083f",
   "metadata": {},
   "source": [
    "Exemplo de como fitar um classificador gradient boost com 100 stumps como weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf66829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba5c4aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=1.0, \n",
    "    max_depth=1, \n",
    "    random_state=0\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be9907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2d76d",
   "metadata": {},
   "source": [
    "## Hiperparâmetros do AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276821ae",
   "metadata": {},
   "source": [
    "- n_estimators: Número de árvores a serem treinadas.\n",
    "- learnig_rate: multiplicador da contribuição de cada árvore na obtenção da resposta.\n",
    "- max_depth: Profundidade máxima de cada árvore.\n",
    "- max_leaf_nodes: Número máximo de nós de cada árvore.\n",
    "- loss: Função que será usada para otimizar as árvores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d00f5",
   "metadata": {},
   "source": [
    "## GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo anterior (np_estimator e learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6548416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8603689515156953"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "est = GradientBoostingRegressor()\n",
    "\n",
    "params = {\n",
    "    'n_estimators': np.arange(1, 501, 20),\n",
    "    'learning_rate': np.linspace(0.001, 1.0, 10)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator = est,\n",
    "    param_grid = params,\n",
    "    scoring = 'r2',\n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c95fc",
   "metadata": {},
   "source": [
    "### Qual a maior diferença entre os dois algoritmos Stochastic GBM e o GBM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d9173",
   "metadata": {},
   "source": [
    "A principal diferença é que no Stochastic GBM é a incorporação da aleatoriedade como parte integral do processo. Em cada iteração uma sub-amostra de base de treino é extraída de maneira aleatória (sem reposição) da base de treino completa. Etapa que não existe no GBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
